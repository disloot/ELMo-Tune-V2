import os
import re
from openai import OpenAI
from utils.constants import EMBEDDING_MODEL, LLM_MODEL, RAG, OPENAI_API_KEY, OPENAI_API_BASE, EMBEDDING_API_KEY, EMBEDDING_API_BASE
from utils.utils import log_gpt_response, log_update
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# Environment variables
if not OPENAI_API_KEY:
    raise ValueError("Error: OPENAI_API_KEY is not set. Please check your .env file or environment variables.")

spec_client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_API_BASE,
)

client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_API_BASE,
)

if 'llama' in LLM_MODEL:
    client = OpenAI(
        api_key=os.getenv("HUGGING_FACE_KEY"),
        base_url = "https://api-inference.huggingface.co/v1/",
    )

embeddings = OpenAIEmbeddings(
    model=EMBEDDING_MODEL,
    openai_api_key=EMBEDDING_API_KEY,
    openai_api_base=EMBEDDING_API_BASE,
)

def request_gpt_rag(system_content, user_contents, assistant_content, temperature):
    '''
    Function to make an API call to GPT-4

    Parameters:
    - system_content: string containing the system information
    - user_contents: list of strings containing the user inputs
    - assistant_content: list of strings containing the assistant responses
    - temperature: Float (0-1) controlling GPT-4's output randomness.

    Returns:
    - matches: string containing the options file generated by GPT-4
    '''
    log_update("[GPTR] Using RAG")
    print("[GPTR] Using RAG")
    # Load vectorstore and create retriever
    vectorstore = FAISS.load_local("vectorstore_2", embeddings=embeddings, allow_dangerous_deserialization=True)
    retriever = vectorstore.as_retriever()

    # Create LLM and RAG chains
    llm = ChatOpenAI(
        model=LLM_MODEL, 
        temperature=temperature,
        openai_api_key=OPENAI_API_KEY,
        openai_api_base=OPENAI_API_BASE,
    )

    # Separate User content
    last_user_content = user_contents[-1]
    user_contents = user_contents[:-1]

    # Create chat prompt template for system and user content
    prompt = [
        ("system", system_content + "\n\n{context}"),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}")
    ]
    
    # Now append user and assistant content
    history = []
    if assistant_content:
        for i in range(max(len(user_contents), len(assistant_content))):
            if i < len(user_contents):
                history.append(HumanMessage(content=user_contents[i]))
            if i < len(assistant_content):
                history.append(AIMessage(content=assistant_content[i]))
    else:
        for content in user_contents:
            history.append(HumanMessage(content=content))

    # Create prompt template and RAG
    prompt_template = ChatPromptTemplate.from_messages(prompt)
    qna_chain = create_stuff_documents_chain(llm, prompt_template)
    rag_chain = create_retrieval_chain(retriever, qna_chain)
    
    # Set user question and ask the RAG
    inputs = {
        "history": history,
        "input": last_user_content
    }
    response = rag_chain.invoke(inputs)
    answer = response.get("answer", "")
    
    # Use for Logging the request 
    messages = [("system", system_content)]
    if assistant_content:
        for i in range(max(len(user_contents), len(assistant_content))):
            if i < len(user_contents):
                messages.append(("user", user_contents[i]))
            if i < len(assistant_content):
                messages.append(("assistant", assistant_content[i]))
    else:
        for content in user_contents:
            messages.append(("user", content))
    messages.append(("user", last_user_content))
    log_gpt_response(messages, answer)
    
    # Use regex to extract content between code blocks if present
    matches = re.match(r"(.*?)```(.*?)```(.*)", answer, re.DOTALL)
    
    if matches:
        return matches
    else:
        # Handle invalid response
        with open("invalid_assistant_reply.txt", "a") as file:
            file.write(answer + "\n\n" + "-" * 150 + "\n\n")
        return None


def request_gpt(system_content, user_contents, assistant_content, temperature):
    '''
    Function to make an API call to GPT-4

    Parameters:
    - system_content: string containing the system information
    - chunk_string: string containing the chunk of the options file
    - previous_option_files: list of tuples containing the previous option files and their benchmark results
    - temperature: Float (0-1) controlling GPT-4's output randomness.
    - average_cpu_used: Float indicating average CPU usage (default -1.0).
    - average_mem_used: Float indicating average memory usage (default -1.0).
    - test_name: String stating the benchmark test.

    Returns:
    - matches: string containing the options file generated by GPT-4
    '''

    if RAG:
        return request_gpt_rag(system_content, user_contents, assistant_content, temperature)

    messages = [{"role": "user", "content": system_content}]

    if assistant_content:
        for i in range(max(len(user_contents), len(assistant_content))):
            if i < len(user_contents):
                messages.append({"role": "user", "content": user_contents[i]})
            if i < len(assistant_content):
                messages.append({"role": "assistant", "content": assistant_content[i]})
    else:
        for content in user_contents:
            messages.append({"role": "user", "content": content})


    # Assuming 'client' is already defined and authenticated for GPT-4 API access
    # completion = client.chat.completions.create(
    #     model=LLM_MODEL,
    #     messages=messages,
    #     temperature=temperature,
    #     max_completion_tokens=4096,
    #     frequency_penalty=0,
    #     presence_penalty=0,
    # )

    completion = client.chat.completions.create(
        model=LLM_MODEL,
        messages=messages,
    )

    # Extract the assistant's reply
    assistant_reply = completion.choices[0].message.content
    matches = re.match("([\s\S]*)```([\s\S]*)```([\s\S]*)", assistant_reply)

    # Log the request and response
    log_gpt_response(messages, assistant_reply)

    # Check if result is good
    if matches is not None:
        return matches 

    # Invalid response
    with open("invalid_assistant_reply.txt", "a") as file:
        file.write(assistant_reply + "\n\n" + "-" * 150 + "\n\n")
    return None

def send_gpt_request(system_contents, user_contents, temperature):
    '''
    Function to send a request to GPT-4

    Parameters:
    - user_contents: list of strings containing the user inputs
    - temperature: Float (0-1) controlling GPT-4's output randomness.

    Returns:
    - matches: string containing the options file generated by GPT-4
    '''

    messages = [{"role": "user", "content": system_contents}, 
                {"role": "user", "content": user_contents}]

    # completion = client.chat.completions.create(
    #     model=LLM_MODEL,
    #     messages=messages,
    #     temperature=1,
    #     max_completion_tokens=4096,
    #     frequency_penalty=0,
    #     presence_penalty=0,
    # )

    completion = client.chat.completions.create(
        model=LLM_MODEL,
        messages=messages,
    )

    # Extract the assistant's reply
    assistant_reply = completion.choices[0].message.content
    log_gpt_response(messages, assistant_reply)

    return assistant_reply

def request_gpt_with_structured_output(system_content, user_contents, assistant_content, response_format, temperature):
    '''
    Function to make an API call to GPT-4

    Parameters:
    - system_content: string containing the system information
    - chunk_string: string containing the chunk of the options file
    - previous_option_files: list of tuples containing the previous option files and their benchmark results
    - temperature: Float (0-1) controlling GPT-4's output randomness.
    - average_cpu_used: Float indicating average CPU usage (default -1.0).
    - average_mem_used: Float indicating average memory usage (default -1.0).
    - test_name: String stating the benchmark test.

    Returns:
    - response: python dictionary containing the JSON response from GPT-4
    '''
    
    messages = [{"role": "system", "content": "You are a helpful assistant."}]
    if system_content is not None:
        messages.append({"role": "user", "content": system_content})

    if assistant_content:
        for i in range(max(len(user_contents), len(assistant_content))):
            if i < len(user_contents):
                messages.append({"role": "user", "content": user_contents[i]})
            if i < len(assistant_content):
                messages.append({"role": "assistant", "content": assistant_content[i]})
    else:
        for content in user_contents:
            messages.append({"role": "user", "content": content})

    we_did_not_specify_stop_tokens = True
    
    # DeepSeek and some other models don't support the beta parse API
    is_deepseek = "deepseek" in LLM_MODEL.lower()
    
    if not is_deepseek:
        try:
            response = spec_client.beta.chat.completions.parse(
                model=LLM_MODEL,
                messages=messages,
                temperature=temperature,
                response_format=response_format,
            )

            if response.choices[0].finish_reason == "length":
                raise Exception("The conversation was too long for the context window, resulting in incomplete JSON")
            if response.choices[0].message.refusal is not None:
                raise Exception(f"The OpenAI safety system refused the request and generated a refusal instead. response.choices[0].message.refusal")
            if response.choices[0].finish_reason == "content_filter":
                raise Exception("The model's output included restricted content, so the generation of JSON was halted and may be partial")
            
            return response.choices[0].message.parsed
            
        except Exception as e:
            log_update(f"[GPT] Structured output failed, falling back to manual JSON parsing: {e}")
            print(f"[GPT] Structured output failed, falling back to manual JSON parsing: {e}")
    
    # Fallback to manual JSON parsing or direct use for DeepSeek
    if messages[0]["role"] == "system":
        messages[0]["content"] += "\nIMPORTANT: You MUST respond in JSON format matching the expected schema."
    else:
        messages.insert(0, {"role": "system", "content": "You are a helpful assistant. IMPORTANT: You MUST respond in JSON format matching the expected schema."})
    
    try:
        completion = spec_client.chat.completions.create(
            model=LLM_MODEL,
            messages=messages,
            temperature=temperature,
            response_format={"type": "json_object"}
        )
        
        json_str = completion.choices[0].message.content
        return response_format.model_validate_json(json_str)
    except Exception as fallback_e:
        log_update(f"[GPT] Fallback JSON parsing also failed: {fallback_e}")
        print(f"[GPT] Fallback JSON parsing also failed: {fallback_e}")
        raise fallback_e